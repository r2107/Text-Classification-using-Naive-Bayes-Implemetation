{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set of stop words from nltk\n",
    "import nltk\n",
    "import os\n",
    "from os import listdir\n",
    "#from nltk.corpus import stopwords\n",
    "#stopwordsset = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing set of StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsset = {'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    " 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    " 'each', 'few', 'for', 'from', 'further', \n",
    " 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    " 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    " 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    " 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    " 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    " '4th', '5th', '6th', '7th', '8th', '9th', '10th'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Document Files into Train and Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19892 19892\n"
     ]
    }
   ],
   "source": [
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "path = r'C:\\Users\\MrLather\\Docsspace\\20_newsgroups'\n",
    "pathOfAllFiles = []\n",
    "Features = []\n",
    "for f in listdir(path):\n",
    "    Features.append(f)\n",
    "Y = []\n",
    "c = 0\n",
    "for r,d,f in os.walk(path):\n",
    "    for file in f:\n",
    "        p = r + '\\\\' + file\n",
    "        pathOfAllFiles.append(p)\n",
    "        Y.append(Features[c - 1])\n",
    "    c += 1\n",
    "print(len(Y), len(pathOfAllFiles))\n",
    "doc_train, doc_test, Y_train, Y_test = train_test_split(pathOfAllFiles, Y, random_state=0, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting meaningfull words to Train and Test the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "import string \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Funtion to return lemmatized word\n",
    "def lemmatize(tup):\n",
    "    word = tup[0]\n",
    "    tag = tup[1]\n",
    "    wnl = WordNetLemmatizer()\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return wnl.lemmatize(word, pos='v')\n",
    "    elif tag.startswith('VB'):\n",
    "        return wnl.lemmatize(word, pos='v')\n",
    "    elif tag.startswith('RB'):\n",
    "        return wnl.lemmatize(word, pos='a')\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wnl.lemmatize(word, pos='a')\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Funtion to return only meaningfull words for training\n",
    "def tokenize(wordlist):\n",
    "    word = []\n",
    "    for w in wordlist:\n",
    "        # removing punctuations from the word\n",
    "        w = ''.join(c for c in w if c not in string.punctuation)\n",
    "        # selecting only words of length > 2 and containing only alphabets\n",
    "        if w.isalpha() and w not in stopwordsset and len(w) > 2: \n",
    "            word.append(lemmatize(pos_tag(word_tokenize(w.lower()))[0]))\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the frequency of selected words in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = {} # to store all words with their frequency\n",
    "Xtraindata = {} # to store all words frequency document wise\n",
    "for filepath in doc_train:\n",
    "    # read the document ans store in a string\n",
    "    stri = open(filepath, \"r\").read()\n",
    "    # storing all meaningfull words in a list\n",
    "    words = tokenize(stri.split(' '))\n",
    "    Xtraindata[filepath] = words\n",
    "    for w in words:\n",
    "        if w in wordlist:\n",
    "            wordlist[w] = wordlist[w] + 1\n",
    "        else:\n",
    "            wordlist[w] = 1\n",
    "#print(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Dictionary to select most frequently occuring words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "feature = {} # to store the features names with its index\n",
    "# li contains sorted list of tuples (key,value) from dictionary\n",
    "li = sorted(wordlist.items(), key = operator.itemgetter(1))\n",
    "l = len(li)\n",
    "n = 4000\n",
    "for i in range(n):\n",
    "    feature[li[l - i - 1][0]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Data to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = []\n",
    "for filepath in doc_train:\n",
    "    temp = [0 for _ in range(n)]\n",
    "    for w in Xtraindata[filepath]:\n",
    "        if w in feature:\n",
    "            temp[feature[w]] += 1\n",
    "    Xtrain.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984818\n"
     ]
    }
   ],
   "source": [
    "print(sum([sum(Xtrain[i]) for i in range(len(Xtrain))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = []\n",
    "for filepath in doc_test:\n",
    "    temp = [0 for _ in range(4000)]\n",
    "    stri = open(filepath, \"r\").read()\n",
    "    words = tokenize(stri.split(' '))\n",
    "    for w in words:\n",
    "        if w in feature:\n",
    "            temp[feature[w]] += 1\n",
    "    Xtest.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using InBuilt NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(Xtrain,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Score of InBuilt NaiveBayes :  0.6412628192238086\n"
     ]
    }
   ],
   "source": [
    "#y_pred = clf.predict(X_test)\n",
    "print(\"Testing Score of InBuilt NaiveBayes : \", clf.score(Xtest, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score of InBuilt NaiveBayes :  0.6848314230176286\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Score of InBuilt NaiveBayes : \", clf.score(Xtrain, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Xtrain = np.array(Xtrain)\n",
    "Y_train = np.array(Y_train)\n",
    "Xtest = np.array(Xtest)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our own NaiveBayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will fit the training data and return a dictionary storing the various counts\n",
    "def fit(X_train, Y_train):\n",
    "    dic = {}\n",
    "    classes = set(Y_train)\n",
    "    dic[\"total_data_points\"] = len(Y_train)\n",
    "    for curr_class in classes:\n",
    "        dic[curr_class] = {}\n",
    "        features = X_train.shape[1]\n",
    "        curr_class_rows = (Y_train == curr_class) # gives us true false values array acc to the condition only if Ytrain is numpy array\n",
    "        X_train_current = X_train[curr_class_rows]\n",
    "        Y_train_current = Y_train[curr_class_rows]\n",
    "        dic[curr_class][\"total\"] = len(Y_train_current)\n",
    "        for j in range(features):\n",
    "            dic[curr_class][j] = {}\n",
    "            posval = set(X_train[:, j])\n",
    "            for k in posval:\n",
    "                dic[curr_class][j][k] = (X_train_current[:, j] == k).sum() # gives us sum of all true values satisfying the condition\n",
    "    return dic\n",
    "\n",
    "# Return the log values of probabilities Using the Bayes Theorem\n",
    "def probability(dic, xi, curr_class):\n",
    "    #curr_class_prob = (dic[curr_class][\"total\"] / dic[\"total_data_points\"])\n",
    "    # the above probability calculations are now converted to log values to avoid very smaller results\n",
    "    curr_class_prob = np.log(dic[curr_class][\"total\"]) - np.log(dic[\"total_data_points\"])\n",
    "    #print(dic[curr_class])\n",
    "    for j in dic[curr_class]:\n",
    "        if j != \"total\":\n",
    "            num = dic[curr_class][j][xi[j]] + 1\n",
    "            den = dic[curr_class][\"total\"] + len(dic[curr_class][j])\n",
    "            curr_class_prob = curr_class_prob + np.log(num) - np.log(den)\n",
    "    return curr_class_prob\n",
    "\n",
    "# This will Predict results for single row of testing data and return the resultant class\n",
    "def predSingle(dic, xi):\n",
    "    classes = dic.keys()\n",
    "    #print(classes)\n",
    "    firstRun = True\n",
    "    for curr_class in classes:\n",
    "        if curr_class != \"total_data_points\":\n",
    "            prob_curr_class = probability(dic, xi, curr_class)\n",
    "            if (firstRun or prob_curr_class > maxp):\n",
    "                maxp = prob_curr_class\n",
    "                maxclass = curr_class\n",
    "                firstRun = False\n",
    "    return maxclass\n",
    "\n",
    "# This function will predict the results and return them as an array\n",
    "def predict(X_test, result):\n",
    "    y_pred = []\n",
    "    for i in range(len(X_test)):\n",
    "        y_pred.append(predSingle(result, X_test[i]))\n",
    "    return y_pred\n",
    "\n",
    "# This function will return the score by comparing the actual results with the predicting results\n",
    "def score(y_pred, Y_test):\n",
    "    score = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == Y_test[i]:\n",
    "            score = score + 1\n",
    "    score = score / len(y_pred)\n",
    "    return score    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making continuous data labelled to fit into our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(column, middle):\n",
    "    # we divide the column values into 4 ranges i.e by dividing the line with 3 points midddle point is the mean\n",
    "    first_point = 0.5 * middle\n",
    "    third_point = 1.5 * middle\n",
    "    for i in range(len(column)):\n",
    "        if column[i] < first_point:\n",
    "            column[i] = 0\n",
    "        elif column[i] < middle:\n",
    "            column[i] = 1\n",
    "        elif column[i] < third_point:\n",
    "            column[i] = 2\n",
    "        else:\n",
    "            column[i] = 3\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Xtrain[0])):\n",
    "    middle = (Xtrain[:,i].mean() + Xtest[:,i].mean()) / 2\n",
    "    Xtrain[:,i] = label(Xtrain[:,i], middle)\n",
    "    Xtest[:,i] = label(Xtest[:,i], middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = fit(np.array(Xtrain), np.array(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(np.array(Xtest), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Score of our algorithm is :  0.6155238286748441\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Score of our algorithm is : \", score(y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Classification Report and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification_Report : -\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.79      0.77      0.78       233\n",
      "           comp.graphics       0.71      0.44      0.55       257\n",
      " comp.os.ms-windows.misc       0.75      0.65      0.70       247\n",
      "comp.sys.ibm.pc.hardware       0.78      0.64      0.70       240\n",
      "   comp.sys.mac.hardware       0.79      0.73      0.76       235\n",
      "          comp.windows.x       0.54      0.56      0.55       242\n",
      "            misc.forsale       0.48      0.83      0.61       258\n",
      "               rec.autos       0.74      0.59      0.66       278\n",
      "         rec.motorcycles       0.90      0.74      0.81       257\n",
      "      rec.sport.baseball       0.95      0.71      0.81       240\n",
      "        rec.sport.hockey       0.75      0.57      0.65       234\n",
      "               sci.crypt       0.90      0.81      0.85       247\n",
      "         sci.electronics       0.71      0.56      0.62       264\n",
      "                 sci.med       0.93      0.59      0.72       251\n",
      "               sci.space       0.88      0.62      0.73       247\n",
      "  soc.religion.christian       0.96      0.64      0.77       261\n",
      "      talk.politics.guns       0.84      0.67      0.74       243\n",
      "   talk.politics.mideast       0.44      0.20      0.28       287\n",
      "      talk.politics.misc       0.20      0.98      0.33       245\n",
      "      talk.religion.misc       0.00      0.00      0.00       207\n",
      "\n",
      "             avg / total       0.71      0.62      0.63      4973\n",
      "\n",
      "##############################\n",
      "\n",
      "Confusion_Matrix : -\n",
      "[[180   1   0   0   1   3   5   5   5   1   4   1   1   1   0   4   5  16\n",
      "    0   0]\n",
      " [  2 114  22  12   7  47  29   1   0   0   1   2   5   1   3   0   1   1\n",
      "    8   1]\n",
      " [  1  11 161  10   1  25  16   2   0   0   0   4   2   0   2   0   0   1\n",
      "    8   3]\n",
      " [  0   2  16 153  14   6  40   2   1   0   0   1   3   0   0   0   0   0\n",
      "    2   0]\n",
      " [  1   2   4   9 172   1  22   3   1   0   0   2   7   1   1   0   0   0\n",
      "    9   0]\n",
      " [  0  10   4   1   2 136   7   0   1   0   1   1   5   1   1   0   0   1\n",
      "   71   0]\n",
      " [  1   0   2   6   4   0 214   5   4   0   2   0   7   0   2   0   0   2\n",
      "    9   0]\n",
      " [  3   0   0   0   2   0  19 164   5   0   0   0   6   1   3   0   4   2\n",
      "   69   0]\n",
      " [  5   1   1   0   0   1  12  10 190   0   0   1   1   0   1   0   3   1\n",
      "   30   0]\n",
      " [  1   0   1   0   1   0  18   2   0 171  30   0   0   1   0   0   1   2\n",
      "   12   0]\n",
      " [  1   0   0   0   0   1   2   0   0   6 133   0   0   0   0   0   0   0\n",
      "   91   0]\n",
      " [  0   2   0   0   1  10   6   0   1   0   0 199   6   0   0   0   2   1\n",
      "   19   0]\n",
      " [  0   4   2   5   6   4  29   7   0   0   1   8 147   1   6   0   1   2\n",
      "   41   0]\n",
      " [  2   4   1   0   3   4   7  14   1   0   0   0   5 147   1   1   1  11\n",
      "   49   0]\n",
      " [  2   9   0   0   4   8  15   2   0   1   2   0  10   1 153   0   2   9\n",
      "   29   0]\n",
      " [  9   0   1   0   0   3   1   3   0   0   2   0   3   1   0 167   2  17\n",
      "   51   1]\n",
      " [  1   1   0   0   0   1   2   1   1   1   1   0   0   0   0   0 162   8\n",
      "   64   0]\n",
      " [ 17   0   0   0   0   0   0   0   0   0   1   3   0   1   0   1   8  58\n",
      "  198   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0   0   0   1   0   1   1   0\n",
      "  240   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  207   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report as cr, confusion_matrix as cm\n",
    "print(\"Classification_Report : -\")\n",
    "print(cr(Y_test, y_pred))\n",
    "print(\"##############################\")\n",
    "print()\n",
    "print(\"Confusion_Matrix : -\")\n",
    "print(cm(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'directorylist = []\\nfor r,d,f in os.walk(path):\\n    c += 1\\n    if(c == 2):\\n        t = \\'\\\\\\'\\n        p = r + t + f[0]\\n        fil = open(p, \"r\")\\n        #print(tokenize(fil.read().split(\\' \\')))\\n        text_lines = fil.readlines()\\n        text_lines = remove_metadata(text_lines)\\n        for i in text_lines:\\n            print(i.strip(\" \"),end = \"\")'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dummy Trial Code\n",
    "#f = open(\"ReadDoc/untitled.txt\", \"r\") # to read the file\n",
    "#dir(f) # to check all methods avail with f\n",
    "#string = f.read() # to read line from the file\n",
    "#from nltk.tokenize import word_tokenize \n",
    "#word_tokens = word_tokenize(string) \n",
    "#for i in word_tokens:\n",
    "    #print(i, end = \"  \")\n",
    "#import os\n",
    "#from os import listdir\n",
    "#directorylist = []\n",
    "#path = r'C:\\Users\\MrLather\\Docsspace\\20_newsgroups'\n",
    "#c = 0\n",
    "# os.walk iterate on all the folders , subfolders inside the path specified including the current folder (20newgroup)\n",
    "#     so if there are 20 folder inside 20news then this loop will run 21 times including the 20newsfolder\n",
    "# r will give names(path) of all 21 folders\n",
    "# d will give a list of names of all folders inside the current r so we get 21 lists(some may be empty)\n",
    "#     where first list will denote the folders inside 20news itself i.e those 20 folders\n",
    "# f will give a list of all the files names that are directly inside the current r not in its subfolder\n",
    "#     so first f contains 0 files in this case while second f contain all files in the first subfolder of 20news folder\n",
    "'''for r,d,f in os.walk(path):\n",
    "    c += 1\n",
    "    if(c == 1):\n",
    "        print(f)\n",
    "print(c)'''\n",
    "\n",
    "'''for f in listdir(path):\n",
    "    print(f)'''\n",
    "\n",
    "'''directorylist = []\n",
    "for r,d,f in os.walk(path):\n",
    "    c += 1\n",
    "    if(c == 2):\n",
    "        t = '\\\\'\n",
    "        p = r + t + f[0]\n",
    "        fil = open(p, \"r\")\n",
    "        #print(tokenize(fil.read().split(' ')))\n",
    "        text_lines = fil.readlines()\n",
    "        text_lines = remove_metadata(text_lines)\n",
    "        for i in text_lines:\n",
    "            print(i.strip(\" \"),end = \"\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
